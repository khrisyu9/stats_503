{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env pre-setting\n",
    "import os\n",
    "import conda\n",
    "\n",
    "conda_file_dir = conda.__file__\n",
    "conda_dir = conda_file_dir.split('lib')[0]\n",
    "proj_lib = os.path.join(os.path.join(conda_dir, 'share'), 'proj')\n",
    "os.environ[\"PROJ_LIB\"] = proj_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits.basemap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eac0134e9619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.basemap'"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select michigan data\n",
    "# sava it as MI_data.csv\n",
    "# data = pd.read_csv(\"/Users/weijiepan/Desktop/US_Accidents_Dec19.csv\")\n",
    "# MI_data = data[data['State'] == 'MI'] \n",
    "# MI_data.to_csv(\"/Users/weijiepan/Desktop/MI_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/weijiepan/Desktop/MI_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process time \n",
    "data['Start_Time'] = pd.to_datetime(data['Start_Time'])\n",
    "data['End_Time'] = pd.to_datetime(data['End_Time'])\n",
    "data['Year'] = data['Start_Time'].dt.year\n",
    "data['Hour'] = data['Start_Time'].dt.hour\n",
    "data['Day'] = data['Start_Time'].dt.dayofweek\n",
    "data['DayName'] = data['Start_Time'].dt.weekday_name\n",
    "data['Month'] = data['Start_Time'].dt.month\n",
    "\n",
    "# two in 2020 (del them)\n",
    "data = data[(data['Year']>2015) & (data['Year']<2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# location\n",
    "heat_map_data = [[i,j,k] for i,j,k in zip(data.Start_Lat,data.Start_Lng,data.Severity)]\n",
    "\n",
    "# michigan state location\n",
    "latitude = 43.182205\n",
    "longitude = -84.506836\n",
    "# Add incidents to map\n",
    "mi_map = folium.Map(location=[latitude , longitude], zoom_start=7)\n",
    "\n",
    "folium.plugins.HeatMap(heat_map_data,min_opacity = 0.1).add_to(mi_map)\n",
    "mi_map \n",
    "# save the heatmap\n",
    "# mi_map.save(outfile= \"map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time \n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 8))\n",
    "fig.tight_layout(pad=3)\n",
    "\n",
    "hours = data.groupby('Hour').size()\n",
    "ax1.bar(hours.index, hours)\n",
    "ax1.set_xticks(hours.index)\n",
    "ax1.set_xlabel('Factor (Hour)')\n",
    "\n",
    "day_names = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "days = data.groupby('Day').size()\n",
    "ax2.bar(days.index, days)\n",
    "ax2.set_xticks(days.index)\n",
    "ax2.set_xticklabels([day_names[i] for i in days.index])\n",
    "ax2.set_xlabel('Factor (Day)')\n",
    "\n",
    "month_names = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\n",
    "              \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\n",
    "              \"Nov\",\"Dec\"]\n",
    "months = data.groupby('Month').size()\n",
    "ax3.bar(months.index, months)\n",
    "ax3.set_xticks(months.index)\n",
    "ax3.set_xticklabels([month_names[i-1] for i in months.index])\n",
    "ax3.set_xlabel('Factor (Month)')\n",
    "\n",
    "years = data.groupby('Year').size()\n",
    "ax4.bar(years.index, years)\n",
    "ax4.set_xticks(years.index)\n",
    "ax4.set_xlabel('Factor (Year)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(y='Weather_Condition', data=data, order=data['Weather_Condition'].value_counts().iloc[:10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select vars\n",
    "var = ['Start_Lat','Start_Lng',\n",
    "        'Temperature(F)', 'Humidity(%)',\n",
    "       'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing',\n",
    "       'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station',\n",
    "       'Stop', 'Traffic_Calming', 'Traffic_Signal',\n",
    "       'Sunrise_Sunset','Hour','Severity']\n",
    "\n",
    "con_var = ['Start_Lat','Start_Lng',\n",
    "        'Temperature(F)', 'Humidity(%)',\n",
    "       'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "subdata = data[var]\n",
    "\n",
    "# standardize\n",
    "subdata[con_var] = (subdata[con_var]-subdata[con_var].mean())/subdata[con_var].std()\n",
    "\n",
    "# SUNRISE _ SUNSET\n",
    "# day = 1 && night = 0\n",
    "subdata['Sunrise_Sunset'][subdata['Sunrise_Sunset'] == \"Day\"] = 1\n",
    "subdata['Sunrise_Sunset'][subdata['Sunrise_Sunset'] == \"Night\"] = 0\n",
    "\n",
    "# drop na\n",
    "subdata.dropna(axis=0,how='any',inplace=True)\n",
    "\n",
    "# find weather top 10 \n",
    "weather_index = list(data['Weather_Condition'].value_counts()[:10].index)\n",
    "weather_index\n",
    "\n",
    "# select top 10 weather\n",
    "subdata = subdata[subdata['Weather_Condition'].isin(weather_index)]\n",
    "\n",
    "# one - hot encode weather \n",
    "subdata=pd.concat([pd.get_dummies(subdata[\"Weather_Condition\"]),subdata],axis=1)\n",
    "\n",
    "# drop Weather and save\n",
    "subdata = subdata.drop(columns=[\"Weather_Condition\"])\n",
    "#subdata.to_csv(\"/Users/weijiepan/Desktop/MI_data_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    subdata.drop(columns = ['Severity']), subdata['Severity'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMP OF FEATURE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "print(model.feature_importances_) \n",
    "#use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "from sklearn.metrics import accuracy_score \n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV(cv=5,class_weight={2:.08,3:.13,4:.79},max_iter=1000)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(model.predict(X_test)==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=8, criterion='entropy',random_state=1,class_weight={2:.08,3:.13,4:.79})\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=31, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "y_train_nn=pd.get_dummies(y_train)\n",
    "y_test_nn=pd.get_dummies(y_test)\n",
    "model.fit(X_train, y_nn, validation_data=(X_test, y_test_nn), epochs=10,class_weight={0:.0,1:.08,2:.13,3:.79})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
